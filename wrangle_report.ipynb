{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wrangle_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Data Analysis goes thru three steps, gathering, assessing, and cleaning data. The wrangle and analyze project tested these three steps on working through a twitter account data.\n",
    "\n",
    "This project's primary focus is the Twitter account, weratedogs, and their unique rating system, where the rating numerator is not bounded, but the rating denominator must remain 10. First part of Data Analysis is to collect the data,  so data was collected from three sources- a csv file, a url and API. A csv file named twitter-archive-enhanced.csv  provided by Udacity to download and save locally and then import it to Jupyter notebook. The second source was a url where the dataset was stored and needed to use requests libary in python to retrieve data from the HTTP and store it in the file- twitter-archive-enhanced.csv. The third source was the twitter's API, where tweepy libary is used to interact with the API given that a person has a twitter account and is approved to collect data from twitter's API. I used the files provided by Udacity, tweet-json.txt and read this file in Jupyter notebook and created a dataframe by using the following three methods:\n",
    "\n",
    "Json_loads- interprets the Json string in python dictionary. \n",
    "Json_normalize- flattens out the json data structure (tabular data).\n",
    "\n",
    "Then, I assessed the data by using the following python built in methods:\n",
    "df.info()-\n",
    "df.describe()\n",
    "df.head()\n",
    "df.sample()\n",
    "df.value_counts()\n",
    "df.isnull()\n",
    "df.duplicated()\n",
    "\n",
    "There are two methods for assessing the data, one is visual, and one is programmatic. For visual assessment, I used df.head() and df.sample() to look at the columns of the dataframe to assess quality ('Dirty Data') and tidiness ('Messy Data') issues. \n",
    "\n",
    "After assessing all three dataframes, there were eight quality issues and two tidiness issues noted and it is best practice to clean the completeness category of the quality issue and then the tidiness issues before moving onto other types of issues. The main reason for tackling the completeness and tidiness issues first is to ensure that the dataframe is complete and well structured for analysis. \n",
    "\n",
    "However, there are instances where the data cannot be completed, and a few reasons for the incompleteness is the user did not enter the information, or there was no activity on those parts of the attributes of the dataframe. For instance, weratedogs had 2356 tweet_ids, where tweets_list_df has 2354 tweet ids, so there were two tweet_ids missing from the tweets_list_df. I retrieved the missing id numbers with a comparison of weratedogs.tweet_ids and tweets_list_df.tweet_ids and went back to the tweet-json.txt to look for these tweet_ids, and they were not present in the file. Therefore,  my assumption is that these two tweet_ids did not have retweets or favorites. \n",
    "\n",
    "After resolving the eight quality issues and two tidiness issues, the cleaned master dataframe was saved as a csv file in Jupyter notebook for further exploratory data analysis. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
